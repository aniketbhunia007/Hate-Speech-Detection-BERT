{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script was used to scrape data from twitter using IDs given in files NLP+CSS_2016.csv and NAACL_SRW_2016.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tweepy\n",
    "import pprint, time\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the original csv file from the GitHub account, which is in this folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rajat/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# hs = pd.read_csv('./NAACL_SRW_2016.csv', index_col=0, header=None)\n",
    "hs = pd.read_csv('./NLP+CSS_2016.csv',sep = '\\s')\n",
    "hs = hs.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename column headings, reset ID to not be the ID number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hs = hs.rename(columns={'TweetID': 'ID', 'Expert': 'Class'})\n",
    "hs.Class = hs.Class.astype('str')\n",
    "#print(hs.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine how big the dataset is (rows, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6909"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "Query the Twitter API to get the tweets from the IDS. Get various key's from Twitter developer website (dev.twitter.com). Run 167 loops of 100 tweets each: first, subset the dataframe by the 100 tweets of each iteration (line 14). Then, define an object \"tweets\" which actually queries the Twitter API for the IDs (fed in list form) of that subset of tweets. Then add those tweets to an existing list, tweet_list, initialized to an empty list in line 10. Finally, sleep for 60 seconds between each loop to avoid breaking the API rate limit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.08 s, sys: 425 ms, total: 4.5 s\n",
      "Wall time: 1h 11min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "consumer_key = 'hQSfuYsO9ybVNVKPCxURndKQ6'\n",
    "consumer_secret = 'BUbTUcCSOozzU3PzFSbt44A4nWDjWVccAwuyPTgzQo25ImhcsV'\n",
    "access_token = '1030700072755855361-1bl3CyvIAvr8VQ74j368130fq7odUT'\n",
    "access_token_secret = '1cw58KNSxH0VCs0bqZCbYdevCIm6sIpzQLkuBvglqirwQ'\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "tweet_list = []\n",
    "\n",
    "\n",
    "for i in range(0, ceil(len(hs.ID) / 100)):\n",
    "    ids = hs.ID.iloc[i*100 : (i+1)*100]\n",
    "    tweets = api.statuses_lookup(list(ids), True)\n",
    "    tweet_list.extend(list(tweets))\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose one tweet and print an object from that tweet for testing purposes, here the first tweet t and its object \"text.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tweet_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After you've run the querying code for at least three hours, you can create lists of various objects from the tweets. In this function we add all of the attributes we were interested in for this project. To see all of the possible attributes, check out the Twitter API's website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_info(tweet_list):\n",
    "    len_list = len(tweet_list)\n",
    "    tweet = []\n",
    "    id_str = []\n",
    "    favorite_count = []\n",
    "    created_at = []\n",
    "    screen_name = []\n",
    "    location = []\n",
    "    description = []\n",
    "    followers_count = []\n",
    "    hashtags = []\n",
    "    user_mentions = []\n",
    "    for t in tweet_list:\n",
    "        tweet.append(t.text)\n",
    "        id_str.append(t.id_str)\n",
    "        favorite_count.append(t.favorite_count)\n",
    "        created_at.append(t.created_at)\n",
    "        screen_name.append(t.author.screen_name)\n",
    "        location.append(t.author.location)\n",
    "        description.append(t.author.description)\n",
    "        followers_count.append(t.author.followers_count)\n",
    "        hashtags.append(t.entities['hashtags'])\n",
    "        user_mentions.append(t.entities['user_mentions'])\n",
    "    tweet_info = pd.DataFrame({'Tweets': tweet, \"ID\": id_str, \n",
    "                               \"Authors\": screen_name, \"Locations\": location, \n",
    "                               \"Descriptions\": description,\"Follower Count\":followers_count, \n",
    "                               \"Time Tweeted\": created_at, \"User Mentions\": user_mentions, \n",
    "                               \"Hashtags\": hashtags, \"Favorite Count\":favorite_count})\n",
    "    return tweet_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_tweet_info(tweet_list)\n",
    "#df.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And authors. Here I've also included code for how to write the list to a csv file so that we can use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('hatespeech_NLP+CSS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
