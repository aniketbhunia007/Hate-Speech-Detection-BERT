{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Jn1dXT9fmWF"
   },
   "source": [
    "We use the newely tarined embeddings for word2vec transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F52nsROaIfea"
   },
   "source": [
    "There is a change in preprocessing in this notebook, we removed all the numbers from tweets, which helped in training more robust word2vec embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KAfnkMx1_M5B"
   },
   "source": [
    "Analysis on scraped dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3kUTiT2E_M5H"
   },
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cU9l4_D1_M5J"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zCUG0trn_M5R"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 34530,
     "status": "ok",
     "timestamp": 1573424656009,
     "user": {
      "displayName": "Pawar Pranav Dayanand mm17b003",
      "photoUrl": "",
      "userId": "03168935693589294616"
     },
     "user_tz": -330
    },
    "id": "KoieloYD56U0",
    "outputId": "726f8c93-8304-4c25-f467-7b6682460415"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8727,
     "status": "ok",
     "timestamp": 1573424732146,
     "user": {
      "displayName": "Pawar Pranav Dayanand mm17b003",
      "photoUrl": "",
      "userId": "03168935693589294616"
     },
     "user_tz": -330
    },
    "id": "Xtb_csJD_M5X",
    "outputId": "1c2c7e22-6883-4968-ede8-22b598a0bab0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cf_data_1 = pd.read_csv('/content/drive/My Drive/Hate_Speech_Detection_git/data_1/hatespeech_NAACL_SRW.csv',encoding = \"ISO-8859-1\")\n",
    "cf_data_2 = pd.read_csv('/content/drive/My Drive/Hate_Speech_Detection_git/data_1/hatespeech_NLP+CSS.csv')\n",
    "\n",
    "cf_data_3 = pd.read_csv('/content/drive/My Drive/Hate_Speech_Detection_git/data_2/labeled_data.csv',encoding = \"ISO-8859-1\")\n",
    "## this is the scraped data\n",
    "\n",
    "cf_data_3.rename({'Unnamed: 0':'ID','tweet':'Tweets'},axis=1,inplace=True)\n",
    "\n",
    "labels_1 = pd.read_csv('/content/drive/My Drive/Hate_Speech_Detection_git/data_1/NAACL_SRW_2016.csv',header=None,names=['ID','class'])\n",
    "labels_2 = pd.read_csv('/content/drive/My Drive/Hate_Speech_Detection_git/data_1/NLP+CSS_2016.csv',sep='\\s')\n",
    "\n",
    "labels_2.rename({'TweetID':'ID','Expert':'class'},axis=1,inplace=True)\n",
    "\n",
    "cf_data_1.rename({'Unnamed: 0':'index_col'},axis=1,inplace=True)\n",
    "cf_data_2.rename({'Unnamed: 0':'index_col'},axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jvHqjnpba_06"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4omcVwyX_M56"
   },
   "source": [
    "### Function for merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BUC130MI_M58"
   },
   "outputs": [],
   "source": [
    "def label_merging(data, labels):\n",
    "    labels['ID'] = labels['ID'].astype(int)\n",
    "    print(labels['ID'].nunique())\n",
    "    print('Null IDs in data 1 = ' ,data['ID'].isna().sum())\n",
    "    \n",
    "    data['ID'].fillna(0,inplace=True)\n",
    "    data['ID'] = data['ID'].astype(int)\n",
    "    \n",
    "    print('data shape ='  ,data.shape)\n",
    "    print('IDs common in data and labels =',sum(data['ID'].isin(labels['ID'])))\n",
    "    \n",
    "    train = data.merge(labels, on='ID',how='inner')#['class'].isna().sum()\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1015,
     "status": "ok",
     "timestamp": 1573416671961,
     "user": {
      "displayName": "Pawar Pranav Dayanand mm17b003",
      "photoUrl": "",
      "userId": "03168935693589294616"
     },
     "user_tz": -330
    },
    "id": "Mj1bg2HO_M6D",
    "outputId": "ca7a6fb5-dfcc-439f-dc5e-a44bedcbc127",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_1 = label_merging(cf_data_1,labels_1)\n",
    "\n",
    "train_2 = label_merging(cf_data_2, labels_2)\n",
    "\n",
    "train_3 = cf_data_3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8CDfNYIT_M6X"
   },
   "outputs": [],
   "source": [
    "t1 = train_1[['ID','Tweets','class']]\n",
    "t2 = train_2[['ID','Tweets','class']]\n",
    "t3 = train_3[['ID','Tweets','class']]\n",
    "merged = pd.concat([t1,t2,t3],axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G52end2X_M6k"
   },
   "source": [
    "### Target Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "od7YmYLQ_M7k"
   },
   "source": [
    "### Basic Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pVMMYTwGxuNy"
   },
   "outputs": [],
   "source": [
    "train = merged.copy()\n",
    "\n",
    "train.rename(columns={'Tweets':'tweet'},inplace=True)\n",
    "train['tweet'] = train['tweet'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OsHyjVfK_M7u"
   },
   "source": [
    "Every word followed by @ is some twitter ID of an user, which shouldn't be considered in our analysis, so lets do the stemming, where we remove @ alonwith the word followed by it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3003,
     "status": "ok",
     "timestamp": 1573416677968,
     "user": {
      "displayName": "Pawar Pranav Dayanand mm17b003",
      "photoUrl": "",
      "userId": "03168935693589294616"
     },
     "user_tz": -330
    },
    "id": "lOeA8PLt_M7z",
    "outputId": "bc324752-d9f0-4440-9a3f-7e8279ae0f3f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train['tweet'] = train['tweet'].apply(lambda x:' '.join(i for i in [a for a in x.split() if a.find('@')==-1]))\n",
    "train['tweet'] = train['tweet'].apply(lambda x:' '.join(i for i in [a for a in x.split() if a.find('http')==-1]))\n",
    "train['tweet'] = train['tweet'].apply(lambda x:''.join([i for i in x if not i.isdigit()]))\n",
    "train['tweet'] = train['tweet'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "train['tweet'] = train['tweet'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "train['tweet'] = train['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "remove_word = ['rt','mkr','im']\n",
    "train['tweet'] = train['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in remove_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fAUvqGB6_M9A"
   },
   "source": [
    "Doesnt really make sense to remove rare words, i.e. the words with count 1. Because we might lose hateful words this way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7516,
     "status": "ok",
     "timestamp": 1573416682958,
     "user": {
      "displayName": "Pawar Pranav Dayanand mm17b003",
      "photoUrl": "",
      "userId": "03168935693589294616"
     },
     "user_tz": -330
    },
    "id": "w1QGCz-0_M9F",
    "outputId": "10f601c4-8717-4cfc-bb88-c5052a603003"
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "nltk.download('punkt')\n",
    "\n",
    "from textblob import Word\n",
    "nltk.download('wordnet')\n",
    "train['tweet'] = train['tweet'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "train['tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n4Rng-D6_M9a"
   },
   "source": [
    "### Target creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6022,
     "status": "ok",
     "timestamp": 1573416682963,
     "user": {
      "displayName": "Pawar Pranav Dayanand mm17b003",
      "photoUrl": "",
      "userId": "03168935693589294616"
     },
     "user_tz": -330
    },
    "id": "v1h4taDp_M9f",
    "outputId": "1b66286b-39bd-4b1b-e3f1-90d643fc8e9f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train['class'].unique()#.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6123,
     "status": "ok",
     "timestamp": 1573416683286,
     "user": {
      "displayName": "Pawar Pranav Dayanand mm17b003",
      "photoUrl": "",
      "userId": "03168935693589294616"
     },
     "user_tz": -330
    },
    "id": "FJuHE_9K_M9p",
    "outputId": "6a93d18b-9cbd-4c9f-9745-b1b8330065bb"
   },
   "outputs": [],
   "source": [
    "train['class'].replace(['racism', 'sexism',0, 1, 'both', 'none', 'neither',2],['hate','hate','hate','hate','hate','null','null','null'],inplace=True)\n",
    "train['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aE1ArY61XLzf"
   },
   "outputs": [],
   "source": [
    "train['class'].replace(['null','hate'],[0,1],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LJZfrwwBayjo"
   },
   "source": [
    "### Removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FvB8Hfzdaxk4"
   },
   "outputs": [],
   "source": [
    "# hate_text = train[train['class']==1]['tweet']\n",
    "# null_text = train[train['class']==0]['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RgdEHU1vbKPU"
   },
   "outputs": [],
   "source": [
    "# hate_text.to_csv(r'hate_speech.txt', header=None, index=None, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ufDPugVbKLH"
   },
   "outputs": [],
   "source": [
    "# null_text.to_csv(r'null_speech.txt', header=None, index=None, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4000,
     "status": "ok",
     "timestamp": 1573416683291,
     "user": {
      "displayName": "Pawar Pranav Dayanand mm17b003",
      "photoUrl": "",
      "userId": "03168935693589294616"
     },
     "user_tz": -330
    },
    "id": "MJDw5gDDv_aq",
    "outputId": "ba4f4198-dae3-4c99-cfa2-c8c98329b046"
   },
   "outputs": [],
   "source": [
    "sum(train['ID'].value_counts()>1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3511,
     "status": "ok",
     "timestamp": 1573416683292,
     "user": {
      "displayName": "Pawar Pranav Dayanand mm17b003",
      "photoUrl": "",
      "userId": "03168935693589294616"
     },
     "user_tz": -330
    },
    "id": "VTiYUKgowFQ7",
    "outputId": "aed4059c-2294-438d-90b8-97e32696010c"
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sKum_pWvwJGv"
   },
   "outputs": [],
   "source": [
    "train = train.drop_duplicates(subset='ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6e_MEbj28dW3"
   },
   "source": [
    "Please note that we wont be doing any validation, instead will train on the whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oGVi5Hkc_pnv"
   },
   "source": [
    "### Training on new word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10743,
     "status": "ok",
     "timestamp": 1573416725811,
     "user": {
      "displayName": "Pawar Pranav Dayanand mm17b003",
      "photoUrl": "",
      "userId": "03168935693589294616"
     },
     "user_tz": -330
    },
    "id": "-joK9G7y_tqR",
    "outputId": "6c3102e4-a71c-42a3-8845-dcc0c32d5d94"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import logging\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\"/content/drive/My Drive/Hate_Speech_Detection_git/model_transfer_learning.txt\", binary=False)\n",
    "# Above is domain + pretrained embeddings\n",
    "\n",
    "# wv = gensim.models.KeyedVectors.load_word2vec_format(\"/content/drive/My Drive/Hate_Speech_Detection_git/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "# above is only pretrained embeddings\n",
    "\n",
    "wv.init_sims(replace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9l1uIbEc_yWl"
   },
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, post) for post in text_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13080,
     "status": "ok",
     "timestamp": 1573416806696,
     "user": {
      "displayName": "Pawar Pranav Dayanand mm17b003",
      "photoUrl": "",
      "userId": "03168935693589294616"
     },
     "user_tz": -330
    },
    "id": "i7ohGolk_24-",
    "outputId": "2286656f-5f39-4a94-f06a-2bdc1588f3a7"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "    \n",
    "# train_w2v, test_w2v = train_test_split(train, test_size=0.2, random_state = 42)\n",
    "\n",
    "train_w2v = train.copy()\n",
    "\n",
    "# test_tokenized = test_w2v.apply(lambda r: w2v_tokenize_text(r['tweet']), axis=1).values\n",
    "train_tokenized = train_w2v.apply(lambda r: w2v_tokenize_text(r['tweet']), axis=1).values\n",
    "\n",
    "X_train_word_average = word_averaging_list(wv,train_tokenized)\n",
    "# X_test_word_average = word_averaging_list(wv,test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Z3j686dADhf"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score, classification_report, roc_auc_score, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V4FDiF73K7Sc"
   },
   "outputs": [],
   "source": [
    "def model_training(clf, x_t, y_t, x_v=None , y_v=None ,task='binary:logistic'):\n",
    "    clf.fit(x_t,y_t)\n",
    "    print('training accuracy', clf.score(x_t,y_t))\n",
    "    \n",
    "    #  we are not really validating anything here so lets just comment these lines out\n",
    "    # if task=='binary:logistic':\n",
    "    #   print('validation accuracy', clf.score(x_v,y_v))\n",
    "    #   print('validation f1_score',f1_score(clf.predict(x_v),y_v))\n",
    "    #   print('validation roc_auc score',roc_auc_score(y_v,clf.predict_proba(x_v)[::,-1]))\n",
    "    #   print('confusion matrix \\n',confusion_matrix(y_v, clf.predict(x_v)))\n",
    "    \n",
    "    # if task=='reg:linear':\n",
    "    #   print('validation r2_score', clf.score(x_v,y_v))\n",
    "    #   print('validation MSE',mean_squared_error(clf.predict(x_v),y_v))\n",
    "\n",
    "            \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 823230,
     "status": "ok",
     "timestamp": 1573417728088,
     "user": {
      "displayName": "Pawar Pranav Dayanand mm17b003",
      "photoUrl": "",
      "userId": "03168935693589294616"
     },
     "user_tz": -330
    },
    "id": "II5GAU_0mS2L",
    "outputId": "f8a70a62-1a7c-4214-a999-f754a2738603"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "xgb_w2v = XGBClassifier(n_estimators=500, max_depth=5,learning_rate=0.1,scale_pos_weight=1.4266790777602751)\n",
    "lgr_w2v = LogisticRegression(n_jobs=1)\n",
    "\n",
    "model_training(xgb_w2v,X_train_word_average,train_w2v['class'])\n",
    "\n",
    "# model_training(lgr_w2v,X_train_word_average,train_w2v['class'],X_test_word_average,test_w2v['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dg_Nn2e4KkiH"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Logistic Regression LogisticRegression(n_jobs=1)  Domain + pretrained EMbeddings\n",
    "# training accuracy 0.8715668902670548\n",
    "# validation accuracy 0.8672487977727158\n",
    "# validation f1_score 0.8934051417538867\n",
    "# validation roc_auc score 0.9390565768416776\n",
    "# confusion matrix \n",
    "#  [[2457  475]\n",
    "#  [ 574 4396]]\n",
    "\n",
    "# XGBoost Domain + Pretrained Embeddings\n",
    "# training accuracy 0.9944310846728263\n",
    "# validation accuracy 0.8772462667679068\n",
    "# validation f1_score 0.9021190716448032\n",
    "# validation roc_auc score 0.950407012333208\n",
    "# confusion matrix \n",
    "#  [[2462  470]\n",
    "#  [ 500 4470]]\n",
    "\n",
    "# Logistic Regression LogisticRegression(n_jobs=1)  Only Pretrained EMbeddings\n",
    "# training accuracy 0.8596696620680927\n",
    "# validation accuracy 0.8535813718046065\n",
    "# validation f1_score 0.8819749056411302\n",
    "# validation roc_auc score 0.9265340679822454\n",
    "# confusion matrix \n",
    "#  [[2422  510]\n",
    "#  [ 647 4323]]\n",
    "# CPU times: user 2.16 s, sys: 162 ms, total: 2.33 s\n",
    "# Wall time: 2.28 s\n",
    "\n",
    "# XGBoost + Pretrained Embeddings\n",
    "# training accuracy 0.9898114162764207\n",
    "# validation accuracy 0.8599088838268792\n",
    "# validation f1_score 0.8885084097089335\n",
    "# validation roc_auc score 0.935837432507734\n",
    "# confusion matrix \n",
    "#  [[2384  548]\n",
    "#  [ 559 4411]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 779,
     "status": "ok",
     "timestamp": 1573418548030,
     "user": {
      "displayName": "Pawar Pranav Dayanand mm17b003",
      "photoUrl": "",
      "userId": "03168935693589294616"
     },
     "user_tz": -330
    },
    "id": "1-_uJHnA9hEY",
    "outputId": "186e8b49-f4b7-4178-b5c0-cce0eaf21633"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "#save model\n",
    "joblib.dump(xgb_w2v, 'xgb_deployement_trained_on_whole_dataset.pkl') \n",
    "\n",
    "# #load saved model\n",
    "# xgb = joblib.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xLUsalQl9g-o"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CfeDcEvHakKX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "XGBoost_deployement.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
